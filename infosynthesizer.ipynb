{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ† Capstone Project: InfoSynthesizer\n## The Context-Aware Research Assistant\n\n**Track:** Concierge Agents\n**Author:** Saravana Prashanth K\n\n---\n\n### Project Overview\n**The Problem:** Professionals waste hours switching between technical documentation and market news to understand a new topic. Furthermore, generic AI summaries don't know *who* you areâ€”giving the same explanation to a CEO as they do to a Developer.\n\n**The Solution:** **InfoSynthesizer**. A multi-agent system that:\n1.  **Reads your Persona:** Uses Memory to understand your job title and interests.\n2.  **Parallellizes Research:** Uses a `ParallelAgent` to scout Technical and Market news simultaneously.\n3.  **Synthesizes:** Uses a Writer agent to compile a tailored executive brief.\n\n### Key Concepts Applied\nThis project demonstrates mastery of the following **5-Day Intensive** concepts:\n1.  **Multi-Agent Architectures:** Using `SequentialAgent` orchestrating a nested `ParallelAgent`.\n2.  **Tools:** Utilizing `Google Search` for real-time grounding.\n3.  **State & Memory:** Implementing `InMemorySessionService` and `preload_memory` for context-aware responses.","metadata":{}},{"cell_type":"code","source":"# --- STEP 1: SETUP & AUTHENTICATION ---\n# 1. Authenticate using Kaggle Secrets\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Authentication Successful\")\nexcept Exception as e:\n    print(f\"âŒ Error: {e}. Please ensure 'GOOGLE_API_KEY' is in your Add-ons -> Secrets.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:38:52.046684Z","iopub.execute_input":"2025-11-29T22:38:52.047683Z","iopub.status.idle":"2025-11-29T22:38:52.227884Z","shell.execute_reply.started":"2025-11-29T22:38:52.047641Z","shell.execute_reply":"2025-11-29T22:38:52.226463Z"}},"outputs":[{"name":"stdout","text":"âœ… Authentication Successful\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Importing ADK components\n\nimport os\nimport asyncio\nfrom kaggle_secrets import UserSecretsClient\nfrom google.genai import types\nfrom google.adk.agents import Agent, LlmAgent, ParallelAgent, SequentialAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.memory import InMemoryMemoryService\nfrom google.adk.tools import google_search, preload_memory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:38:54.036615Z","iopub.execute_input":"2025-11-29T22:38:54.036929Z","iopub.status.idle":"2025-11-29T22:39:51.476623Z","shell.execute_reply.started":"2025-11-29T22:38:54.036902Z","shell.execute_reply":"2025-11-29T22:39:51.475123Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 2. Configure the Model\n\nretry_config = types.HttpRetryOptions(attempts=3, exp_base=2, initial_delay=1, http_status_codes=[429, 500, 503])\nmodel = Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config)\nprint(\"âœ… Model Configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:03.497572Z","iopub.execute_input":"2025-11-29T22:40:03.498928Z","iopub.status.idle":"2025-11-29T22:40:03.505072Z","shell.execute_reply.started":"2025-11-29T22:40:03.498895Z","shell.execute_reply":"2025-11-29T22:40:03.503880Z"}},"outputs":[{"name":"stdout","text":"âœ… Model Configured\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Agent Architecture\n\nTo solve this problem efficiently, we aren't just using one agent. We are building a team.\n\n**The Team Structure:**\n1.  **ContextManager (LlmAgent):** The \"Memory Handler\". It looks at past interactions to define *who* the user is.\n2.  **ResearchTeam (ParallelAgent):**\n    * *TechResearcher:* Googles technical specs and docs.\n    * *MarketResearcher:* Googles stock trends and business news.\n    * *Why Parallel?* These tasks don't depend on each other. Running them together cuts latency in half.\n3.  **Writer (Agent):** Takes the context + tech data + market data and writes the final report.","metadata":{}},{"cell_type":"code","source":"# --- STEP 2: DEFINE SPECIALIST AGENTS ---\n\n# 1. User Context Agent (The \"Memory\" Handler)\ncontext_agent = LlmAgent(\n    name=\"ContextManager\",\n    model=model,\n    instruction=\"You are a context manager. Use 'preload_memory' to find out details about the user (e.g., their job, interests). Summarize who the user is in one sentence so the researchers know how to tailor the content.\",\n    tools=[preload_memory]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:06.315434Z","iopub.execute_input":"2025-11-29T22:40:06.315784Z","iopub.status.idle":"2025-11-29T22:40:06.321230Z","shell.execute_reply.started":"2025-11-29T22:40:06.315760Z","shell.execute_reply":"2025-11-29T22:40:06.320201Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 2. Parallel Research Team (The \"Workers\")\n\ntech_researcher = Agent(\n    name=\"TechResearcher\",\n    model=model,\n    instruction=\"Search for the latest TECHNICAL developments regarding the user's query. Focus on specs, code, architecture, and release notes.\",\n    tools=[google_search],\n    output_key=\"tech_findings\" # Saving state for the writer later\n)\n\nmarket_researcher = Agent(\n    name=\"MarketResearcher\",\n    model=model,\n    instruction=\"Search for the latest MARKET/BUSINESS news regarding the user's query. Focus on stock prices, acquisitions, competitor analysis, and business impact.\",\n    tools=[google_search],\n    output_key=\"market_findings\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:09.177698Z","iopub.execute_input":"2025-11-29T22:40:09.178127Z","iopub.status.idle":"2025-11-29T22:40:09.187058Z","shell.execute_reply.started":"2025-11-29T22:40:09.178051Z","shell.execute_reply":"2025-11-29T22:40:09.185580Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Group them into a Parallel Agent to run simultaneously\nresearch_team = ParallelAgent(\n    name=\"ResearchTeam\",\n    sub_agents=[tech_researcher, market_researcher]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:12.216216Z","iopub.execute_input":"2025-11-29T22:40:12.216553Z","iopub.status.idle":"2025-11-29T22:40:12.222395Z","shell.execute_reply.started":"2025-11-29T22:40:12.216529Z","shell.execute_reply":"2025-11-29T22:40:12.220626Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 3. Writer Agent (The \"Synthesizer\")\nwriter_agent = Agent(\n    name=\"Writer\",\n    model=model,\n    instruction=\"\"\"\n    You are an executive writer. \n    1. Read the User Context provided by the ContextManager.\n    2. Read the Tech Findings: {tech_findings}\n    3. Read the Market Findings: {market_findings}\n    \n    Synthesize a cohesive report answering the user's original query. \n    \n    IMPORTANT: Tailor the tone specifically to the user's background found in the context. \n    If they are technical, use jargon. If they are business-focused, focus on ROI.\n    \n    Format the output in clean Markdown with bold headers.\n    \"\"\",\n)\n\nprint(\"âœ… Agents Initialised\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:15.189042Z","iopub.execute_input":"2025-11-29T22:40:15.189384Z","iopub.status.idle":"2025-11-29T22:40:15.195383Z","shell.execute_reply.started":"2025-11-29T22:40:15.189358Z","shell.execute_reply":"2025-11-29T22:40:15.194414Z"}},"outputs":[{"name":"stdout","text":"âœ… Agents Initialised\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# --- STEP 3: ORCHESTRATION ---\n\nroot_agent = SequentialAgent(\n    name=\"InfoSynthesizer\",\n    sub_agents=[context_agent, research_team, writer_agent]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:18.019587Z","iopub.execute_input":"2025-11-29T22:40:18.019890Z","iopub.status.idle":"2025-11-29T22:40:18.025587Z","shell.execute_reply.started":"2025-11-29T22:40:18.019861Z","shell.execute_reply":"2025-11-29T22:40:18.024248Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# --- STEP 4: RUNTIME & MEMORY ---\n\nsession_service = InMemorySessionService()\nmemory_service = InMemoryMemoryService() \n\nrunner = Runner(\n    agent=root_agent,\n    app_name=\"InfoSynthesizerApp\",\n    session_service=session_service,\n    memory_service=memory_service\n)\n\nprint(\"âœ… Workflow Orchestrated & Runner Ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:20.294594Z","iopub.execute_input":"2025-11-29T22:40:20.294900Z","iopub.status.idle":"2025-11-29T22:40:20.301645Z","shell.execute_reply.started":"2025-11-29T22:40:20.294877Z","shell.execute_reply":"2025-11-29T22:40:20.300410Z"}},"outputs":[{"name":"stdout","text":"âœ… Workflow Orchestrated & Runner Ready\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### ðŸš€ Live Demonstration\n\nIn this demo, we will simulate a realistic usage pattern:\n1.  **Memory Seeding:** We will tell the system *who* we are (simulating a previous conversation).\n2.  **The Task:** We will ask a complex question about **\"Gemini 1.5\"**.\n3.  **The Result:** Watch how the agent finds both technical specs and business news, then writes a report tailored to a **Data Scientist**.","metadata":{}},{"cell_type":"code","source":"async def run_demo():\n    print(\"--- ðŸ§  PHASE 1: MEMORY INGESTION ---\")\n    user_id = \"user1\"\n    intro_session_id = \"intro_session\"\n    task_session_id = \"task_session\" # Define the new session ID\n    \n    # 1. Create the intro session\n    await session_service.create_session(\n        app_name=\"InfoSynthesizerApp\", \n        user_id=user_id, \n        session_id=intro_session_id\n    )\n    \n    # We simulate the user introducing themselves.\n    persona_text = \"I am a Marketing Executive interested in brand safety and ROI.\"\n    print(f\"User: '{persona_text}'\")\n    \n    # Run the agent silently to populate the session history\n    async for event in runner.run_async(\n        user_id=user_id, \n        session_id=intro_session_id, \n        new_message=types.Content(parts=[types.Part(text=persona_text)])\n    ):\n        pass \n\n    print(\"System: Storing persona in memory...\")\n    \n    # 2. Retrieve the populated session object\n    session = await session_service.get_session(\n        app_name=\"InfoSynthesizerApp\", \n        user_id=user_id, \n        session_id=intro_session_id\n    )\n    \n    # 3. Add the session history to long-term memory\n    await memory_service.add_session_to_memory(session)\n    \n    print(\"\\n--- ðŸš€ PHASE 2: EXECUTING THE TASK ---\")\n    query = \"Tell me about the latest updates in Gemini 1.5.\"\n    \n    print(f\"User Query: {query}\")\n    print(\"...Agent is thinking (running Context check, Parallel Research, and Drafting)...\")\n    print(\"...This might take 10-20 seconds as it browses the web...\")\n    \n    await session_service.create_session(\n        app_name=\"InfoSynthesizerApp\", \n        user_id=user_id, \n        session_id=task_session_id\n    )\n    \n    # 4. Run the actual task in the NEW session\n    async for event in runner.run_async(\n        user_id=user_id, \n        session_id=task_session_id, \n        new_message=types.Content(parts=[types.Part(text=query)])\n    ):\n        if event.is_final_response() and event.content:\n             print(f\"\\nðŸ¤– FINAL REPORT:\\n\\n{event.content.parts[0].text}\")\n\n# Execute the demo\nawait run_demo()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T22:40:23.627751Z","iopub.execute_input":"2025-11-29T22:40:23.628027Z","iopub.status.idle":"2025-11-29T22:40:40.614970Z","shell.execute_reply.started":"2025-11-29T22:40:23.628009Z","shell.execute_reply":"2025-11-29T22:40:40.614018Z"}},"outputs":[{"name":"stdout","text":"--- ðŸ§  PHASE 1: MEMORY INGESTION ---\nUser: 'I am a Marketing Executive interested in brand safety and ROI.'\nSystem: Storing persona in memory...\n\n--- ðŸš€ PHASE 2: EXECUTING THE TASK ---\nUser Query: Tell me about the latest updates in Gemini 1.5.\n...Agent is thinking (running Context check, Parallel Research, and Drafting)...\n...This might take 10-20 seconds as it browses the web...\n\nðŸ¤– FINAL REPORT:\n\nThe user is a Marketing Executive interested in brand safety and ROI, and has previously received information about AI's role in these areas.\n\nðŸ¤– FINAL REPORT:\n\nGemini 1.5 Pro continues to be a rapidly evolving AI model, with recent updates focusing on enhanced capabilities, improved performance, and broader accessibility for businesses. Key developments include significant price reductions for API usage, making advanced AI more cost-effective for marketing applications.\n\nHere are some of the latest updates and their impact for a Marketing Executive focused on brand safety and ROI:\n\n**Enhanced Capabilities and Business Impact:**\n\n*   **Massive Context Window:** Gemini 1.5 Pro boasts an unprecedented context window of up to 1 million tokens, extendable to 2 million. This allows it to process and analyze vast amounts of informationâ€”equivalent to hours of video, extensive audio files, or hundreds of thousands of wordsâ€”in a single instance. For marketers, this translates to deeper insights from large datasets, such as comprehensive competitor analysis, extensive customer feedback analysis, or deep dives into market research reports. This capability can significantly boost ROI by enabling more informed strategic decisions based on a holistic understanding of available data.\n*   **Advanced Multimodal Understanding:** The model's ability to process text, images, audio, and video natively enhances its utility. This means Gemini 1.5 Pro can analyze video content, understand audio inputs directly, and reason across different modalities simultaneously. For marketing, this opens doors for more sophisticated content analysis, personalized user interactions based on voice commands, and improved search optimization through audio understanding.\n*   **Improved Performance and Efficiency:** Recent updates have led to performance optimizations, faster output, and lower latency. This means quicker turnaround times for marketing tasks, from content creation to data analysis. The introduction of more efficient model architectures, like Mixture of Experts (MoE), also contributes to better performance and scalability.\n*   **Enhanced Information Retrieval:** Gemini 1.5 Pro offers superior information retrieval capabilities, acting like a \"supercharged search engine\" that can pinpoint relevant details within massive datasets. This is invaluable for market research, competitive intelligence, and identifying key trends or customer feedback efficiently.\n*   **Cost Reductions and Increased Rate Limits:** Google has significantly reduced API pricing for Gemini 1.5 series models, with substantial price cuts on input and output tokens. Additionally, rate limits have been increased, allowing for more complex AI applications and higher volumes of processing. These factors directly contribute to improved ROI by lowering the cost of AI implementation for marketing initiatives.\n*   **Integration into Google Workspace:** Gemini for Google Workspace provides a seamless way to integrate AI capabilities into tools like Google Sheets and Slides, enhancing efficiency in data organization, project management, and content creation.\n\n**Brand Safety and ROI Considerations:**\n\n*   **Brand Safety Features:** While Gemini models are designed with safety in mind, the latest updates also allow developers more control over safety filters. By default, Google may not apply AI content safety filters, enabling developers to configure them based on their specific requirements. This provides flexibility in managing brand safety, allowing for tailored approaches to content moderation and risk mitigation. Gemini can be configured to act as a safety filter, helping to mitigate brand safety risks from user or tool inputs, and model outputs.\n*   **Data Privacy:** For enterprise users, Gemini Advanced ensures that data is not used for training models, offering enterprise-grade data protection. This is crucial for maintaining the confidentiality of marketing campaign data and customer information.\n*   **Competitive Edge:** The enhanced capabilities of Gemini 1.5 Pro can provide a significant competitive advantage by enabling more sophisticated marketing intelligence, deeper customer engagement through personalization, and faster market entry for new products and campaigns.\n\nIn summary, Gemini 1.5 Pro's advancements in context window, multimodality, performance, and cost-efficiency offer significant potential to enhance marketing strategies, drive better ROI, and ensure brand safety through customizable controls.\n\nðŸ¤– FINAL REPORT:\n\nHello! Here are the latest technical updates on Gemini 1.5, focusing on specs, architecture, and release notes:\n\n**Gemini 1.5 Pro:**\n\n*   **Architecture:** Gemini 1.5 Pro utilizes a sparse Mixture-of-Experts (MoE) Transformer architecture, building upon Gemini 1.0's multimodal capabilities. This MoE approach allows for dynamic routing to activate specific subsets of parameters (\"experts\") based on the input data, leading to increased computational efficiency and scalability.\n*   **Context Window:** A significant advancement is its context window, which can extend up to 2 million tokens for enterprise customers, and has been tested experimentally up to 10 million tokens. This allows it to process and reason over vast amounts of information, such as an hour of video, 11 hours of audio, or codebases exceeding 30,000 lines.\n*   **Performance & Capabilities:** Gemini 1.5 Pro performs comparably to Gemini 1.0 Ultra, with notable improvements in areas like math, long-context understanding, and vision tasks. It excels in multimodal reasoning, processing text, images, audio, and video simultaneously. It also demonstrates strong performance in tasks such as translation, coding, and complex reasoning.\n*   **Release & Updates:**\n    *   Gemini 1.5 Pro was initially previewed in February 2024 and became generally available in May 2024.\n    *   Updated production-ready versions, `gemini-1.5-pro-002` and `gemini-1.5-flash-002`, were released on September 24, 2024, offering improved overall quality, faster output, lower latency, and significantly reduced pricing (over 50% reduction for prompts under 128K tokens).\n    *   Rate limits have been increased, with up to 1,000 requests per minute on the paid tier for Gemini 1.5 Pro.\n\n**Gemini 1.5 Flash:**\n\n*   **Architecture:** Gemini 1.5 Flash is a lighter model derived from Gemini 1.5 Pro, optimized for speed and efficiency. It's also based on the MoE architecture but distilled for efficient deployment.\n*   **Context Window:** It features a 1 million token context window by default, enabling it to process extensive data like an hour of video or 11 hours of audio.\n*   **Performance & Capabilities:** Gemini 1.5 Flash is designed for high-volume, high-frequency tasks where speed and cost-efficiency are paramount. It offers real-time capabilities and minimal latency, making it suitable for applications like chatbots and web apps. While generally faster and cheaper than Pro, it may be less performant on highly complex reasoning tasks.\n*   **Release & Updates:**\n    *   Gemini 1.5 Flash was introduced in May 2024.\n    *   Stable versions `gemini-1.5-flash-002` and `gemini-1.5-flash-8b-001` were released in September 2024.\n    *   It has seen performance increases across text and multimodal use cases and offers significantly reduced API pricing and increased rate limits (up to 2,000 requests per minute on the paid tier).\n\n**Key Technical Improvements Across Gemini 1.5 Series:**\n\n*   **Long Context:** The extended context window is a major breakthrough, allowing for deeper understanding and analysis of lengthy documents, code, audio, and video.\n*   **Multimodality:** Seamless integration and reasoning across text, images, audio, and video inputs are core to the Gemini 1.5 models.\n*   **Efficiency:** The MoE architecture and distillation processes enhance computational efficiency, reducing training compute and serving costs.\n*   **Cost Reduction:** Significant price reductions on API usage for both Pro and Flash models make these advanced capabilities more accessible.\n\nThese updates indicate a strong focus on enhancing performance, expanding capabilities, and improving cost-effectiveness for a wide range of applications.\n\nðŸ¤– FINAL REPORT:\n\nHere's a summary of the latest updates to Gemini 1.5, tailored for your focus as a Marketing Executive on brand safety and ROI:\n\n**Gemini 1.5: Enhanced Capabilities for Marketing Impact**\n\nRecent advancements in Gemini 1.5 are significantly expanding its utility for marketing professionals, directly impacting both strategic decision-making and return on investment.\n\n**Key Enhancements and Their Marketing Benefits:**\n\n*   **Massive Context Window for Deeper Insights:** Gemini 1.5 Pro now offers an exceptional context window of up to 1 million tokens, with experimental capabilities reaching 10 million tokens. This means it can process and analyze vast datasets â€“ think hours of video, extensive audio recordings, or entire codebases â€“ in a single instance. For marketers, this unlocks unprecedented opportunities for in-depth competitor analysis, comprehensive customer feedback analysis, and detailed market research. This capability directly translates to more informed strategic decisions, leading to a higher ROI.\n*   **Advanced Multimodal Understanding for Richer Engagement:** The model's native ability to process text, images, audio, and video simultaneously is a game-changer. This allows for sophisticated analysis of video content, direct understanding of audio inputs (like customer voice feedback), and cross-modal reasoning. For marketing, this opens doors to richer content analysis, highly personalized user interactions via voice, and improved search optimization through audio comprehension.\n*   **Improved Performance and Cost Efficiency:** Gemini 1.5 Pro and Gemini 1.5 Flash have seen significant performance optimizations, leading to faster output and lower latency. This accelerates marketing tasks from content creation to data analysis. Crucially, Google has implemented substantial price reductions for API usage, with over a 50% cost decrease for prompts under 128K tokens. Combined with increased rate limits (up to 1,000 requests per minute for Pro, and 2,000 for Flash on paid tiers), these updates make advanced AI more accessible and cost-effective, directly boosting ROI.\n*   **Streamlined Information Retrieval:** Gemini 1.5 acts as a \"supercharged search engine\" within your data. Its enhanced information retrieval capabilities allow for rapid identification of key trends, customer sentiments, and competitive intelligence from massive datasets, saving valuable time and resources.\n*   **Seamless Integration:** Gemini for Google Workspace offers integrated AI capabilities within tools like Sheets and Slides, further enhancing efficiency in data organization, project management, and content creation workflows.\n\n**Focus on Brand Safety and ROI:**\n\n*   **Customizable Brand Safety Controls:** While Gemini is built with safety in mind, recent updates provide developers with greater control over safety filters. This allows for tailored approaches to content moderation and risk mitigation, enabling you to configure safety settings to align with your specific brand guidelines and protect your brand reputation. Gemini can actively function as a safety filter, helping to mitigate risks from user inputs and model outputs.\n*   **Enterprise-Grade Data Privacy:** For enterprise users, Gemini Advanced ensures that your data is not used for model training, providing robust protection for sensitive marketing campaign data and customer information.\n*   **Competitive Advantage:** By leveraging these enhanced capabilities, you can gain a significant competitive edge through more sophisticated marketing intelligence, deeper customer engagement via personalization, and faster market responsiveness.\n\nIn essence, the latest Gemini 1.5 updates offer a powerful combination of advanced analytical capabilities, cost efficiencies, and customizable controls that are designed to enhance marketing strategies, drive measurable ROI, and safeguard brand integrity.\n","output_type":"stream"}],"execution_count":10}]}